#!/usr/bin/env python3

def word_frequencies(filename):
    return {}

def main():
    pass

if __name__ == "__main__":
    main()

# Create function word_frequencies that gets a filename as a parameter and returns a dict 
# with the word frequencies. 
# In the dictionary the keys are the words and the corresponding values are the number of times
# that word occurred in the file specified by the function parameter. 
# Read all the lines from the file and split the lines into words using the split() method. 
# Further, 
# remove punctuation from the ends of words using the strip("""!"#$%&'()*,-./:;?@[]_""") method call.

# Test this function in the main function using the file alice.txt. 
# In the output, there should be a word and its count per line separated by a tab:

#The     64
#Project 83
#Gutenberg   26
#EBook   3
#of      303
import re
ls_lines = [] # list that will store all the lines of the file
ls_words = [] # list that will store the individual words
with open("/Users/Mamba/Library/Application Support/tmc/vscode/mooc-data-analysis-with-python-2021/part02-e04_word_frequencies/src/alice.txt", "r") as f:
    for line in f:
        line = line.strip() # remove leading and trailing spaces (if any)
        
        words = re.split(r'\s', line) # split the lines into words 
        for i, j in enumerate(words):
            if not re.findall(r'[\w]$', j): # si le mot ne finit pas par une lettre ou chiffre  
                words[i] = j.strip("""!"#$%&'()*,-./:;?@[]_""") # remove punctuation from ends & starts of words
       
        ls_lines.append(line)
        ls_words.append(words) # words est une liste avec tous les mots d'une ligne
        # donc len(ls_words) == len(ls_lines)

 
#print(ls_lines[0])
#print(ls_words[0])
print(1.85*3)  


# count each unique word by line
# use a counter variable everytime i detect an occuurrence of a unique word