#!/usr/bin/env python3

def word_frequencies(filename):
    return {}

def main():
    pass

if __name__ == "__main__":
    main()

# Create function word_frequencies that gets a filename as a parameter and returns a dict 
# with the word frequencies. 
# In the dictionary the keys are the words and the corresponding values are the number of times
# that word occurred in the file specified by the function parameter. 
# Read all the lines from the file and split the lines into words using the split() method. 
# Further, 
# remove punctuation from the ends of words using the strip("""!"#$%&'()*,-./:;?@[]_""") method call.

# Test this function in the main function using the file alice.txt. 
# In the output, there should be a word and its count per line separated by a tab:

#The     64
#Project 83
#Gutenberg   26
#EBook   3
#of      303
import re
ls_lines = [] # list that will store all the lines of the file
ls_words = [] # list that will store the individual words
with open("/Users/Mamba/Library/Application Support/tmc/vscode/mooc-data-analysis-with-python-2021/part02-e04_word_frequencies/src/alice.txt", "r") as f:
    for line in f:
        line = line.strip() # remove leading and trailing spaces (if any)
        
        words = re.split(r'\s', line) # split the lines into words 
        for i, j in enumerate(words):
            if not re.findall(r'[\w]$', j): # si le mot ne finit pas par une lettre ou chiffre  
                words[i] = j.strip("""!"#$%&'()*,-./:;?@[]_""") # remove punctuation from ends & starts of words
       
        ls_lines.append(line)
        ls_words.append(words) # words est une liste avec tous les mots d'une ligne
        # donc len(ls_words) == len(ls_lines)

 
#print(ls_lines[0])
#print(ls_words[0])
#print(len(ls_lines))
#print(len(ls_words[0]))

# A PARTIR D'ICI JE COMPLIQUE TROP LES CHOSES ----------
dic = {}
for i in ls_words:
    for k, j in enumerate(i):
        if j not in dic and j: # s'il n'est pas dans le dico et s'il n'est pas vide
            dic[j] = 1
        elif j in dic and j:
            #print(j)
            #print(dic[j])
            key_index = list(dic.keys()).index(j)
            #print(key_index) # va remplacer k
            #print(list(dic.values())[key_index]) # je ne dois pas utiliser k mais l'index de keys
            #print(list(dic.keys())[key_index])
            

            #print(dic.values())
            dic[j] = list(dic.values())[key_index] + 1
            print(dic)




#counter = 0
#d = {} # dic that will store the words count
#for k, l in enumerate(ls_words): # me permet d'entrer dans chaque liste de ls_words
    # identifie chaque mot unique de chaque ligne (utiliser words pour ça)
    # si un mot se retrouve dans l'une des lignes suivantes ajoute au compte
 #   print(l)
  #  for m in range(len(l)): # parcourt chaque mot à l'intérieur de l
   #     if l[m] != l[m+1]:
            # mettre l[m] dans un dico comme key et value = 1
            # faire pareil pour l[m+1]
            #d = {} # populate this dictionary with the conditions above
    #        counter += 1
     #       d[l[m]] = counter
      #      print(d)
       #     print(list(d.keys())[0], "\t", list(d.values())[0]) # how the final output shoul looks like
            #print(list(d.values())[0])
        #else:
            # update la key de 1
            # create a variable that will count the number of occurrences
         #   new_d = {}
        #print(m)
        #print(l[m])
   
    #print(1.85*3)  


# count each unique word by line
# use a counter variable everytime i detect an occuurrence of a unique word